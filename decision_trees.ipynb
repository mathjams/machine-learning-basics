{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKFFVpL9tHS4lc5YF7rLW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/machine-learning-basics/blob/main/decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S0iBsqId0Fh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from public_tests import *\n",
        "from utils import *\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How decision trees work is that we can first use one-hot encoding to describe a certain set of features, make a \"tree\" splitting on certain features in order to categorize.\n",
        "\n",
        "The entropy of a node is $$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$ which measures how homogenous it is (labels of ex poiosonous vs not)."
      ],
      "metadata": {
        "id": "5RLUmyOQeGxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def compute_entropy(y):\n",
        "    \"\"\"\n",
        "    Computes the entropy for\n",
        "\n",
        "    Args:\n",
        "       y (ndarray): Numpy array indicating whether each example at a node is\n",
        "           edible (`1`) or poisonous (`0`)\n",
        "\n",
        "    Returns:\n",
        "        entropy (float): Entropy at that node\n",
        "\n",
        "    \"\"\"\n",
        "    entropy = 0.\n",
        "    if len(y)==0:\n",
        "        return entropy\n",
        "    else:\n",
        "        p1=sum(y)/len(y)\n",
        "        if p1==0 or p1==1:\n",
        "            return entropy\n",
        "        else:\n",
        "            entropy=-p1*math.log2(p1)-(1-p1)*math.log2(1-p1)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "jF_MLyJSefqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can code to split the dataset based on whether it has a certain features (split based on 0 or 1)."
      ],
      "metadata": {
        "id": "Mne4b0B1el9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, node_indices, feature):\n",
        "    \"\"\"\n",
        "    Splits the data at the given node into\n",
        "    left and right branches\n",
        "\n",
        "    Args:\n",
        "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
        "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
        "        feature (int):           Index of feature to split on\n",
        "\n",
        "    Returns:\n",
        "        left_indices (list):     Indices with feature value == 1\n",
        "        right_indices (list):    Indices with feature value == 0\n",
        "    \"\"\"\n",
        "\n",
        "    left_indices = []\n",
        "    right_indices = []\n",
        "    samples=X.shape[0]\n",
        "    features=X.shape[1]\n",
        "    for i in range(len(node_indices)):\n",
        "        if X[node_indices[i]][feature]==1:\n",
        "            left_indices.append(node_indices[i])\n",
        "        else:\n",
        "            right_indices.append(node_indices[i])\n",
        "    return left_indices, right_indices"
      ],
      "metadata": {
        "id": "4FxcnUwYes5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can define information gain as $$H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$, where $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportions split to the left and the right respectively. This is how much the entropy changes by splitting (and how much more homogenous it becomes)."
      ],
      "metadata": {
        "id": "g6TD1z6ie4yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_information_gain(X, y, node_indices, feature):\n",
        "\n",
        "    \"\"\"\n",
        "    Compute the information of splitting the node on a given feature\n",
        "\n",
        "    Args:\n",
        "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
        "        y (array like):         list or ndarray with n_samples containing the target variable\n",
        "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
        "        feature (int):           Index of feature to split on\n",
        "\n",
        "    Returns:\n",
        "        cost (float):        Cost computed\n",
        "\n",
        "    \"\"\"\n",
        "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
        "    X_node, y_node = X[node_indices], y[node_indices]\n",
        "    X_left, y_left = X[left_indices], y[left_indices]\n",
        "    X_right, y_right = X[right_indices], y[right_indices]\n",
        "\n",
        "    information_gain = 0\n",
        "\n",
        "    information_gain=compute_entropy(y_node)-(len(X_left)/len(X_node)*compute_entropy(y_left)+len(X_right)/len(X_node)*compute_entropy(y_right))\n",
        "\n",
        "    return information_gain"
      ],
      "metadata": {
        "id": "yX1Pce_ZfIul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, at each step we want to make it as homogenous as possible. So, we look for the best feature to split on."
      ],
      "metadata": {
        "id": "Y7Ni0e0XfZwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_split(X, y, node_indices):\n",
        "    \"\"\"\n",
        "    Returns the optimal feature and threshold value\n",
        "    to split the node data\n",
        "\n",
        "    Args:\n",
        "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
        "        y (array like):         list or ndarray with n_samples containing the target variable\n",
        "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
        "\n",
        "    Returns:\n",
        "        best_feature (int):     The index of the best feature to split\n",
        "    \"\"\"\n",
        "\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    best_feature = -1\n",
        "\n",
        "    for i in range(num_features):\n",
        "        if compute_information_gain(X, y, node_indices, i)>0 and best_feature==-1:\n",
        "            best_feature=i\n",
        "        if compute_information_gain(X, y, node_indices, i)>0 and best_feature!=-1 and compute_information_gain(X, y, node_indices, i)>compute_information_gain(X, y, node_indices, best_feature):\n",
        "            best_feature=i\n",
        "\n",
        "    return best_feature"
      ],
      "metadata": {
        "id": "mBrTHUEwfec9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for the final tree:"
      ],
      "metadata": {
        "id": "YyJ5dkH2fo3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = []\n",
        "\n",
        "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
        "    \"\"\"\n",
        "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
        "    This function just prints the tree.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
        "        y (array like):         list or ndarray with n_samples containing the target variable\n",
        "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
        "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
        "        max_depth (int):        Max depth of the resulting tree.\n",
        "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Maximum depth reached - stop splitting\n",
        "    if current_depth == max_depth:\n",
        "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
        "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
        "        return\n",
        "\n",
        "    # Otherwise, get best split and split the data\n",
        "    # Get the best feature and threshold at this node\n",
        "    best_feature = get_best_split(X, y, node_indices)\n",
        "\n",
        "    formatting = \"-\"*current_depth\n",
        "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
        "\n",
        "    # Split the dataset at the best feature\n",
        "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
        "    tree.append((left_indices, right_indices, best_feature))\n",
        "\n",
        "    # continue splitting the left and the right child. Increment current depth\n",
        "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
        "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
      ],
      "metadata": {
        "id": "225Xf63ofoSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}