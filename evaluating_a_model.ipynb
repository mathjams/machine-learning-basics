{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTzZ4bQp5iu4iUdu8K+F04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/machine-learning-basics/blob/main/evaluating_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfUmqF1DN9lQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu,linear\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "from public_tests_a1 import *\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "from assigment_utils import *\n",
        "\n",
        "tf.autograph.set_verbosity(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can split our data set into training set, cross validation set, and test set (model is not trained on). The cross validation set can be used to tune parameters like the degree of the polynomial, the training is for tuning parameters $w$ and $b$, and the test can be used to gauge performance of the chosen parameters (using the cv set)."
      ],
      "metadata": {
        "id": "hCo9DRKJOHz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=1)"
      ],
      "metadata": {
        "id": "IlZMZt58ORxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate a linear regression model, we can calculate the test error $$ J_\\text{test}(\\mathbf{w},b) =\n",
        "            \\frac{1}{2m_\\text{test}}\\sum_{i=0}^{m_\\text{test}-1} ( f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}_\\text{test}) - y^{(i)}_\\text{test} )^2\n",
        "            \\tag{1}\n",
        "$$\n",
        "\n",
        "The code for it is below."
      ],
      "metadata": {
        "id": "edlHfoLEOWm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_mse(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error on a data set.\n",
        "    Args:\n",
        "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
        "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
        "    Returns:\n",
        "      err: (scalar)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    err = 0.0\n",
        "    for i in range(m):\n",
        "        err+=(y[i]-yhat[i])**2/(2*m)\n",
        "    return(err)"
      ],
      "metadata": {
        "id": "pkY06fySOgBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical data, the error function is this $$ J_{cv} =\\frac{1}{m}\\sum_{i=0}^{m-1}\n",
        "\\begin{cases}\n",
        "    1, & \\text{if $\\hat{y}^{(i)} \\neq y^{(i)}$}\\\\\n",
        "    0, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "with the code below:"
      ],
      "metadata": {
        "id": "RW9NIBLnQtst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_cat_err(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculate the categorization error\n",
        "    Args:\n",
        "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
        "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
        "    Returns:|\n",
        "      cerr: (scalar)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    cerr = 0\n",
        "    for i in range(m):\n",
        "        if y[i]!=yhat[i]:\n",
        "            cerr+=1/m\n",
        "    return(cerr)"
      ],
      "metadata": {
        "id": "omaXDOBmQ11T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evalculate the errors, run the model on both sets of data."
      ],
      "metadata": {
        "id": "RoJAciCdRqwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
        "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))"
      ],
      "metadata": {
        "id": "nHpIcR2FRshp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To regularize data to discourage more advanced models, add the following line to a neural network:"
      ],
      "metadata": {
        "id": "F20VaSN8R1_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l2(0.1))"
      ],
      "metadata": {
        "id": "Fx0a9oSASFsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "model_r = Sequential(\n",
        "    [\n",
        "        Dense(units=120, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "        Dense(units=40, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "        Dense(units=6, activation=\"linear\")\n",
        "\n",
        "    ], name= None\n",
        ")\n",
        "model_r.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n"
      ],
      "metadata": {
        "id": "VyLI2i-PR_MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can test different regularization $\\lambda$s and use our cv sets to choose the best one. Then, to evalulate the actual performance, use the test sets to determine the final error."
      ],
      "metadata": {
        "id": "_p44C6ezSnRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3] #testing many lambdas\n",
        "models=[None] * len(lambdas)\n",
        "\n",
        "for i in range(len(lambdas)):\n",
        "    lambda_ = lambdas[i]\n",
        "    models[i] =  Sequential(\n",
        "        [\n",
        "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
        "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
        "            Dense(classes, activation = 'linear')\n",
        "        ]\n",
        "    )\n",
        "    models[i].compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "    )\n",
        "\n",
        "    models[i].fit(\n",
        "        X_train,y_train,\n",
        "        epochs=1000\n",
        "    )\n",
        "    print(f\"Finished lambda = {lambda_}\")"
      ],
      "metadata": {
        "id": "zx2Drc4ZS1io"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}