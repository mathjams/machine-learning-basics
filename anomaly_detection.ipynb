{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIYsQCwQEi2Qq4NSgvKMdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/machine-learning-basics/blob/main/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyXPAeeC8Jw4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_val = load_data()"
      ],
      "metadata": {
        "id": "BAHTaHOK8RUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at anomaly detection, we can fit the distribution to a Gaussian distribution for the features. The Gaussian distribution is  $$ p(x ; \\mu,\\sigma ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x - \\mu)^2}{2 \\sigma ^2}}. $$ We can calculate the mean and the variance for each feature using $$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$ and $$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$"
      ],
      "metadata": {
        "id": "8a4-vJXo-66i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_gaussian(X):\n",
        "    \"\"\"\n",
        "    Calculates mean and variance of all features\n",
        "    in the dataset\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): (m, n) Data matrix, m samples, n features\n",
        "\n",
        "    Returns:\n",
        "        mu (ndarray): (n,) Mean of all features\n",
        "        var (ndarray): (n,) Variance of all features\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "    mu=np.zeros(n)\n",
        "    var=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        sample=X[:,i]\n",
        "        mui=sum(sample)/len(sample)\n",
        "        vari=0\n",
        "        for j in range(len(sample)):\n",
        "            vari+=(X[j][i]-mui)**2/m\n",
        "        mu[i]=mui\n",
        "        var[i]=vari\n",
        "    return mu, var"
      ],
      "metadata": {
        "id": "kteXFuOwAGRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we set a threshold $\\epsilon$ to determine whether an example is probable or not. We calculate the p values of certain sets of features and if it falls below $\\epsilon$, we call it an abnormality. We can test different values of $\\epsilon$ to find the best threshold.\n",
        "\n",
        "Now, from here we can calculate the $tp, fp$ and $fn$ rates which are true positives, false positive, and false negatives respectively. We can define $prec=\\frac{tp}{tp+fp}$ and $rec=\\frac{tp}{tp+fn}$ and an $F_1$ score as $F_1=\\frac{2\\cdot prec\\cdot rec}{prec+rec}$ or the harmonic mean. We choose the $\\epsilon$ with the highest $F_1$ score."
      ],
      "metadata": {
        "id": "sU8c8rMLARin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_threshold(y_val, p_val):\n",
        "    \"\"\"\n",
        "    Finds the best threshold to use for selecting outliers\n",
        "    based on the results from a validation set (p_val)\n",
        "    and the ground truth (y_val)\n",
        "\n",
        "    Args:\n",
        "        y_val (ndarray): Ground truth on validation set\n",
        "        p_val (ndarray): Results on validation set\n",
        "\n",
        "    Returns:\n",
        "        epsilon (float): Threshold chosen\n",
        "        F1 (float):      F1 score by choosing epsilon as threshold\n",
        "    \"\"\"\n",
        "    best_epsilon = 0\n",
        "    best_F1 = 0\n",
        "    F1 = 0\n",
        "    step_size = (max(p_val) - min(p_val)) / 1000\n",
        "\n",
        "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
        "        tp=0\n",
        "        fp=0\n",
        "        fn=0\n",
        "        for i in range(len(p_val)):\n",
        "            if p_val[i]<epsilon:\n",
        "                if y_val[i]==1:\n",
        "                    tp+=1\n",
        "                if y_val[i]==0:\n",
        "                    fp+=1\n",
        "            if p_val[i]>epsilon and y_val[i]==1:\n",
        "                fn+=1\n",
        "        if tp==0:\n",
        "            F1==0\n",
        "        else:\n",
        "            prec=tp/(tp+fp)\n",
        "            rec=tp/(tp+fn)\n",
        "            F1=2*prec*rec/(prec+rec)\n",
        "        if F1 > best_F1:\n",
        "            best_F1 = F1\n",
        "            best_epsilon = epsilon\n",
        "\n",
        "    return best_epsilon, best_F1"
      ],
      "metadata": {
        "id": "ES1cmaxEBOls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}