{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtkg7qRfgc1FxDF6mldxo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/machine-learning-basics/blob/main/supervised_learning_linear_and_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataset\n",
        "x_train, y_train = load_data()"
      ],
      "metadata": {
        "id": "VHeh3hvM9Fyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ],
      "metadata": {
        "id": "EBKBOE0rAF-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For linear regression, $f_{w,b}=wx+b$ is the function.\n",
        "\n",
        "Want to find the best fit with the smallest cost $$J(w,b)=\\frac{1}{2m}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)})-y^{(i)})^2.$$\n",
        "\n",
        "The following code calculates $J(w,b)$ for a specific training set $x,y$ and parameters $w$ and $b$."
      ],
      "metadata": {
        "id": "bhivU1kx9Wc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of the data. To change the markers to red \"x\",\n",
        "# we used the 'marker' and 'c' parameters\n",
        "plt.scatter(x_train, y_train, marker='x', c='r')\n",
        "\n",
        "# Set the title\n",
        "plt.title(\"Profits vs. Population per city\")\n",
        "# Set the y-axis label\n",
        "plt.ylabel('Profit in $10,000')\n",
        "# Set the x-axis label\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hgm3Zfnf_9Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "        x (ndarray): Shape (m,) Input to the model (ex Population of cities)\n",
        "        y (ndarray): Shape (m,) Output (ex Profit)\n",
        "        w, b (scalar): Parameters of the model\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    m = x.shape[0]\n",
        "    total_cost = 0\n",
        "    for i in range(m):\n",
        "        prediction=w*x[i]+b\n",
        "        cost=(prediction-y[i])**2\n",
        "        total_cost+=cost/(2*m)\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "MBZr-qxu9nbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent slowly changes the parameters $w,b$ until convergence. In particular, they adjust so that $w,b$ approach the local minimum. In particular, $b$ adjusts to\n",
        "$b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}$ and $w$ adjusts to $w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}$.\n",
        "\n",
        "We can calculate $\\frac{\\partial J(w,b)}{\\partial b}=\\frac{1}{m}\\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)})-y^{(i)})$ and $\\frac{\\partial J(w,b)}{\\partial w}=\\frac{1}{m}\\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)})-y^{(i)})(x^{(i)})$.\n",
        "\n",
        "The code below calculates the gradients $\\frac{\\partial J(w,b)}{\\partial b}$ and $\\frac{\\partial J(w,b)}{\\partial w}$ explicitly.\n"
      ],
      "metadata": {
        "id": "timD4p33-FYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: compute_gradient\n",
        "def compute_gradient(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      x (ndarray): Shape (m,) Input to the model (Population of cities)\n",
        "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
        "      w, b (scalar): Parameters of the model\n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
        "     \"\"\"\n",
        "    m = x.shape[0]\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "    for i in range(m):\n",
        "        fwb=w*x[i]+b\n",
        "        dj_dw+=(fwb-y[i])*x[i]/m\n",
        "        dj_db+=(fwb-y[i])/m\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "3Jz4IbPm-IR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x :    (ndarray): Shape (m,)\n",
        "      y :    (ndarray): Shape (m,)\n",
        "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
        "      cost_function: function to compute cost\n",
        "      gradient_function: function to compute the gradient\n",
        "      alpha : (float) Learning rate\n",
        "      num_iters : (int) number of iterations to run gradient descent\n",
        "    Returns\n",
        "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "    m = len(x)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_dw, dj_db = gradient_function(x, y, w, b )\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(x, y, w, b)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            w_history.append(w)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w, b, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "GLd7Ks2i_VNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "dv4GDIcWASPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Logistic Regression, we use the Sigmoid function $$f_{w,b}(x)=g(w\\cdot x+b)=\\frac{1}{1+e^{-(w\\cdot x+b)}}$$.\n",
        "\n",
        "Below we provide a code for the Sigmoid function, which returns all the components of $z$ under $g(z)$."
      ],
      "metadata": {
        "id": "fivmX3z_AVMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        g=np.zeros(z.shape)\n",
        "        if len(z.shape)>1:\n",
        "            for i in range(z.shape[0]):\n",
        "                for j in range(z.shape[1]):\n",
        "                    g[i][j]=1/(1+math.exp(-z[i][j]))\n",
        "        else:\n",
        "            for i in range(z.shape[0]):\n",
        "                g[i]=1/(1+math.exp(-z[i]))\n",
        "        return g\n",
        "    except:\n",
        "        return 1/(1+math.exp(-z))"
      ],
      "metadata": {
        "id": "8hgbsM56AEWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function for a sigmoid function is a bit different, where (note that $w$ is vector with parameters of many different features) $$J(w,b)=\\frac{1}{m}\\sum_{i=0}^{m-1} loss(f_{w,b}(x^{(i)}), y^{(i)})$$ with a differently defined $loss$ function than for linear regression. In this case, $y^{(i)}$ is either $1$ or $0$, discrete values instead of continuous labels. The loss function is defined as $$loss(f_{w,b}(x^{(i)}), y^{(i)})=-y^{(i)}\\log (f_{w,b}(x^{(i)}))-(1-y^{(i)})\\log (1-f_{w,b}(x^{(i)}))$$ where $f_{w,b}(x^{(i)})=g(w\\cdot x^{(i)}+b)$ is the model's prediction.\n",
        "\n",
        "Below computes the cost for training $X$ and outputs $y$ and parameters $w,b$."
      ],
      "metadata": {
        "id": "p3-4xr9lA-65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b, *argv):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      *argv : unused, for compatibility with regularized version below\n",
        "    Returns:\n",
        "      total_cost : (scalar) cost\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    total_cost=0\n",
        "    for i in range(m):\n",
        "        sum=b\n",
        "        for j in range(n):\n",
        "            sum+=X[i][j]*w[j]\n",
        "        fwb=sigmoid(sum)\n",
        "        total_cost+=(-y[i]*math.log(fwb)-(1-y[i])*math.log(1-fwb))/m\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "yhQDXlYIVZEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient function for logistic and linear functions are the same, and so is gradient descent."
      ],
      "metadata": {
        "id": "n8IH-ljIVpMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b, *argv):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      *argv : unused, for compatibility with regularized version below\n",
        "    Returns\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "    for i in range(m):\n",
        "        z_wb = 0\n",
        "        for j in range(n):\n",
        "            z_wb += X[i][j]*w[j]\n",
        "        z_wb += b\n",
        "        f_wb = sigmoid(z_wb)\n",
        "\n",
        "        dj_db += (f_wb-y[i])/m\n",
        "\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j]+(f_wb-y[i])*X[i][j]/m\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "cPwvWSMMVtEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, given a dataset you can use this learned model on a dataset to predict whether it is $0$ or $1$ and select the threshold for doing so."
      ],
      "metadata": {
        "id": "dxNp5To7WE1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "    # Loop over each example\n",
        "    for i in range(m):\n",
        "        z_wb = 0\n",
        "        # Loop over each feature\n",
        "        for j in range(n):\n",
        "            # Add the corresponding term to z_wb\n",
        "            z_wb += X[i][j]*w[j]\n",
        "\n",
        "        # Add bias term\n",
        "        z_wb += b\n",
        "\n",
        "        # Calculate the prediction for this example\n",
        "        f_wb = sigmoid(z_wb)\n",
        "\n",
        "        # Apply the threshold\n",
        "        if f_wb<0.5:\n",
        "            p[i] = 0\n",
        "        else:\n",
        "            p[i]=1\n",
        "    return p"
      ],
      "metadata": {
        "id": "7Vsyml8iWOiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization**"
      ],
      "metadata": {
        "id": "96KlU5FoWXAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid overfitting, we add a regularization term $\\frac{\\lambda}{2m}\\sum_{i=0}^{n-1} w_i^2$ to $J(w,b)$ to decrease the coefficients of features that aren't as important (kind of like pinalize them). For example, we might assign features like $x_i^2$ to make a nonlinear function, but this might not be the best fit (maybe a linear function is the best fit!) and even cause overfitting which makes it harder to generalize.\n",
        "\n",
        "The code below adds regularization."
      ],
      "metadata": {
        "id": "O_2w_oesal4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar, float) Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost : (scalar)     cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost(X, y, w, b)\n",
        "\n",
        "    # You need to calculate this value\n",
        "    reg_cost = 0.\n",
        "\n",
        "    for i in range(n):\n",
        "        reg_cost+=lambda_*w[i]**2/(2*m)\n",
        "\n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + reg_cost\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "Ko37036FbAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization term changes $\\frac{\\partial J(w,b)}{\\partial w_j}$ by a term of $\\frac{\\lambda}{m}w_j$.  \n"
      ],
      "metadata": {
        "id": "JQkf10vdbSfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression with regularization\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar,float)  regularization constant\n",
        "    Returns\n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "\n",
        "    for i in range(n):\n",
        "        dj_dw[i]+=lambda_*w[i]/m\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "dGlSz3_5bfeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}